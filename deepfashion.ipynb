{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOdSeAgpnyZJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CYF7ULaNg_4m",
        "outputId": "6f0c6cad-901c-45b7-d18e-ff06fa68b2c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.10)\n",
            "Requirement already satisfied: opencv-python-headless==4.7.0.72 in /usr/local/lib/python3.11/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: transformers==4.30.2 in /usr/local/lib/python3.11/dist-packages (4.30.2)\n",
            "Requirement already satisfied: streamlit==1.22.0 in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.70.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.12.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.30.2) (4.67.1)\n",
            "Requirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (4.2.2)\n",
            "Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (5.5.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (8.1.8)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (8.6.1)\n",
            "Requirement already satisfied: pandas<3,>=0.25 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (2.2.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (11.1.0)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (18.1.0)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (1.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (2.8.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (8.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (0.10.2)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (5.3)\n",
            "Requirement already satisfied: validators>=0.2 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (0.34.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (3.1.44)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (0.9.1)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (6.4.2)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.11/dist-packages (from streamlit==1.22.0) (6.0.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from altair<5,>=3.2.0->streamlit==1.22.0) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<5,>=3.2.0->streamlit==1.22.0) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<5,>=3.2.0->streamlit==1.22.0) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from altair<5,>=3.2.0->streamlit==1.22.0) (0.12.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19->streamlit==1.22.0) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->streamlit==1.22.0) (3.21.0)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=0.25->streamlit==1.22.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=0.25->streamlit==1.22.0) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.30.2) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->streamlit==1.22.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->streamlit==1.22.0) (2.18.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.22.0) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<5,>=3.2.0->streamlit==1.22.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit==1.22.0) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit==1.22.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit==1.22.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit==1.22.0) (0.23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->streamlit==1.22.0) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5bd0aa93f8e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Import libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Install dependencies\n",
        "!pip install tensorflow==2.12.0 kagglehub opencv-python-headless==4.7.0.72 scikit-learn==1.2.2 transformers==4.30.2 streamlit==1.22.0\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Download FashionGen dataset\n",
        "try:\n",
        "    import kagglehub\n",
        "    path = kagglehub.dataset_download(\"bothin/fashiongen-validation\")\n",
        "    data_dir = \"/content/bothin/fashiongen-validation\"\n",
        "    logger.info(f\"Dataset downloaded to: {data_dir}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Dataset download failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# 1. Enhanced GAN Architecture\n",
        "def build_generator(text_embed_dim=768, latent_dim=100):\n",
        "    \"\"\"Builds a DCGAN-based generator with text conditioning\"\"\"\n",
        "    noise_input = tf.keras.Input(shape=(latent_dim,))\n",
        "    text_input = tf.keras.Input(shape=(text_embed_dim,))\n",
        "\n",
        "    # Concatenate and project to initial dense layer\n",
        "    x = tf.keras.layers.concatenate([noise_input, text_input])\n",
        "    x = tf.keras.layers.Dense(4*4*512, use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Reshape((4, 4, 512))(x)\n",
        "\n",
        "    # Upsampling blocks\n",
        "    x = tf.keras.layers.Conv2DTranspose(256, (5,5), strides=(2,2), padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(128, (5,5), strides=(2,2), padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    # Final output layer\n",
        "    x = tf.keras.layers.Conv2DTranspose(3, (5,5), strides=(2,2), padding='same', activation='tanh')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[noise_input, text_input], outputs=x)\n",
        "\n",
        "def build_discriminator(text_embed_dim=768):\n",
        "    \"\"\"Builds a PatchGAN discriminator with text conditioning\"\"\"\n",
        "    image_input = tf.keras.Input(shape=(128, 128, 3))\n",
        "    text_input = tf.keras.Input(shape=(text_embed_dim,))\n",
        "\n",
        "    # Text processing pathway\n",
        "    text = tf.keras.layers.Dense(512)(text_input)\n",
        "    text = tf.keras.layers.LeakyReLU(0.2)(text)\n",
        "    text = tf.keras.layers.Dense(128 * 128)(text)\n",
        "    text = tf.keras.layers.Reshape((128, 128, 1))(text)\n",
        "\n",
        "    # Image processing pathway\n",
        "    img = tf.keras.layers.Conv2D(64, (5,5), strides=(2,2), padding='same')(image_input)\n",
        "    img = tf.keras.layers.LeakyReLU(0.2)(img)\n",
        "\n",
        "    # Concatenate text and image features\n",
        "    combined = tf.keras.layers.concatenate([img, text])\n",
        "\n",
        "    # Downsampling blocks\n",
        "    x = tf.keras.layers.Conv2D(128, (5,5), strides=(2,2), padding='same')(combined)\n",
        "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(256, (5,5), strides=(2,2), padding='same')(x)\n",
        "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=[image_input, text_input], outputs=x)\n",
        "\n",
        "# 2. Improved Text Processing\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def text_to_embedding(self, text):\n",
        "        \"\"\"Converts text to BERT embeddings with error handling\"\"\"\n",
        "        try:\n",
        "            inputs = self.tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
        "            outputs = self.model(inputs)\n",
        "            return tf.reduce_mean(outputs.last_hidden_state, axis=1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Text processing failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "text_processor = TextProcessor()\n",
        "\n",
        "# 3. Robust Data Pipeline\n",
        "class DataLoader:\n",
        "    def __init__(self, data_dir, img_size=(128, 128)):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def _load_pair(self, img_path, txt_path):\n",
        "        \"\"\"Loads and validates a single image-text pair\"\"\"\n",
        "        try:\n",
        "            # Load image\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                raise ValueError(f\"Failed to read image: {img_path}\")\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            img = (img.astype(np.float32) / 127.5) - 1.0  # Normalize to [-1, 1]\n",
        "\n",
        "            # Load text\n",
        "            with open(txt_path, 'r') as f:\n",
        "                text = f.read().strip()\n",
        "                if not text:\n",
        "                    raise ValueError(f\"Empty text file: {txt_path}\")\n",
        "\n",
        "            return img, text\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Skipping invalid pair {img_path}: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"Loads and validates the entire dataset\"\"\"\n",
        "        images = []\n",
        "        texts = []\n",
        "\n",
        "        for root, _, files in os.walk(self.data_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(\".jpg\"):\n",
        "                    img_path = os.path.join(root, file)\n",
        "                    txt_path = os.path.join(root, file.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "                    if os.path.exists(txt_path):\n",
        "                        img, text = self._load_pair(img_path, txt_path)\n",
        "                        if img is not None and text is not None:\n",
        "                            images.append(img)\n",
        "                            texts.append(text)\n",
        "\n",
        "        logger.info(f\"Successfully loaded {len(images)} valid pairs\")\n",
        "        return np.array(images), np.array(texts)\n",
        "\n",
        "# Load and prepare data\n",
        "try:\n",
        "    loader = DataLoader(data_dir)\n",
        "    images, descriptions = loader.load_dataset()\n",
        "\n",
        "    # Generate text embeddings\n",
        "    text_embeddings = []\n",
        "    valid_indices = []\n",
        "    for idx, desc in enumerate(descriptions):\n",
        "        embedding = text_processor.text_to_embedding(desc)\n",
        "        if embedding is not None:\n",
        "            text_embeddings.append(embedding.numpy()[0])\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "    # Filter out failed embeddings\n",
        "    images = images[valid_indices]\n",
        "    text_embeddings = np.array(text_embeddings)\n",
        "    logger.info(f\"Final dataset shape: Images {images.shape}, Embeddings {text_embeddings.shape}\")\n",
        "\n",
        "    # Split dataset\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        images, text_embeddings,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "except Exception as e:\n",
        "    logger.error(f\"Data preparation failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# 4. Enhanced Training System\n",
        "class GANTrainer:\n",
        "    def __init__(self, generator, discriminator, lr=0.0002, beta=0.5):\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.gan = self._build_gan()\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "\n",
        "    def _build_gan(self):\n",
        "        \"\"\"Constructs combined GAN model\"\"\"\n",
        "        self.discriminator.trainable = False\n",
        "        noise_input = tf.keras.Input(shape=(100,))\n",
        "        text_input = tf.keras.Input(shape=(768,))\n",
        "        generated_image = self.generator([noise_input, text_input])\n",
        "        validity = self.discriminator([generated_image, text_input])\n",
        "        return tf.keras.Model([noise_input, text_input], validity)\n",
        "\n",
        "    def compile_models(self):\n",
        "        \"\"\"Configures model optimizers and losses\"\"\"\n",
        "        optimizer = tf.keras.optimizers.Adam(self.lr, self.beta)\n",
        "\n",
        "        self.discriminator.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.gan.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            optimizer=optimizer\n",
        "        )\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=100, batch_size=32, save_interval=10):\n",
        "        \"\"\"Enhanced training loop with progress tracking\"\"\"\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Select random batch\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_images = X_train[idx]\n",
        "            real_text = y_train[idx]\n",
        "\n",
        "            # Generate fake images\n",
        "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "            gen_images = self.generator.predict([noise, real_text])\n",
        "\n",
        "            # Train discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch([real_images, real_text], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([gen_images, real_text], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "            g_loss = self.gan.train_on_batch([noise, real_text], valid)\n",
        "\n",
        "            # Save progress\n",
        "            if epoch % save_interval == 0:\n",
        "                self._save_weights(epoch)\n",
        "                self._log_progress(epoch, d_loss, g_loss)\n",
        "                self._generate_sample(epoch)\n",
        "\n",
        "        self._save_weights('final')\n",
        "\n",
        "    def _save_weights(self, epoch):\n",
        "        \"\"\"Saves model weights with epoch information\"\"\"\n",
        "        self.generator.save_weights(f'/content/drive/MyDrive/generator_{epoch}.h5')\n",
        "        self.discriminator.save_weights(f'/content/drive/MyDrive/discriminator_{epoch}.h5')\n",
        "\n",
        "    def _log_progress(self, epoch, d_loss, g_loss):\n",
        "        \"\"\"Logs training metrics\"\"\"\n",
        "        logger.info(\n",
        "            f\"Epoch {epoch} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}%] \"\n",
        "            f\"[G loss: {g_loss:.4f}]\"\n",
        "        )\n",
        "\n",
        "    def _generate_sample(self, epoch):\n",
        "        \"\"\"Generates and saves sample images\"\"\"\n",
        "        noise = np.random.normal(0, 1, (1, 100))\n",
        "        sample_text = y_train[np.random.randint(0, y_train.shape[0])]\n",
        "        gen_image = self.generator.predict([noise, sample_text.reshape(1, -1)])\n",
        "        gen_image = (gen_image[0] * 127.5 + 127.5).astype(np.uint8)\n",
        "        cv2.imwrite(f'sample_epoch_{epoch}.png', gen_image)\n",
        "\n",
        "# Initialize and train\n",
        "try:\n",
        "    generator = build_generator()\n",
        "    discriminator = build_discriminator()\n",
        "\n",
        "    trainer = GANTrainer(generator, discriminator)\n",
        "    trainer.compile_models()\n",
        "\n",
        "    logger.info(\"Starting training...\")\n",
        "    trainer.train(X_train, y_train, epochs=100, batch_size=32)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Training failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# 5. Production-Ready Streamlit App\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "# Configuration\n",
        "MODEL_PATH = '/content/drive/MyDrive/generator_final.h5'\n",
        "IMG_SIZE = (128, 128)\n",
        "\n",
        "# Load models\n",
        "@st.cache_resource\n",
        "def load_generator():\n",
        "    generator = build_generator()\n",
        "    generator.load_weights(MODEL_PATH)\n",
        "    return generator\n",
        "\n",
        "generator = load_generator()\n",
        "\n",
        "# App interface\n",
        "st.title(\"ðŸ›ï¸ FashionGen AI\")\n",
        "st.markdown(\"Generate fashion items from text descriptions using AI\")\n",
        "\n",
        "# Input section\n",
        "text_input = st.text_input(\"Describe your fashion item:\",\n",
        "                         placeholder=\"e.g., 'A red floral summer dress with puff sleeves'\")\n",
        "\n",
        "# Generation controls\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    num_images = st.selectbox(\"Number of images\", [1, 2, 4], index=0)\n",
        "with col2:\n",
        "    noise_level = st.slider(\"Creativity level\", 0.0, 1.0, 0.5)\n",
        "\n",
        "# Generate button\n",
        "if st.button(\"âœ¨ Generate Fashion\"):\n",
        "    if text_input:\n",
        "        with st.spinner(\"Generating your fashion items...\"):\n",
        "            try:\n",
        "                # Process text\n",
        "                text_embed = text_processor.text_to_embedding(text_input)\n",
        "                if text_embed is None:\n",
        "                    st.error(\"Failed to process text description\")\n",
        "                    raise\n",
        "\n",
        "                # Generate images\n",
        "                noise = np.random.normal(0, noise_level, (num_images, 100))\n",
        "                generated = generator.predict([noise, text_embed.numpy()])\n",
        "\n",
        "                # Display results\n",
        "                cols = st.columns(num_images)\n",
        "                for i in range(num_images):\n",
        "                    img = (generated[i] * 127.5 + 127.5).astype(np.uint8)\n",
        "                    cols[i].image(img, caption=f\"Variation {i+1}\", use_column_width=True)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Generation failed: {str(e)}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a description\")\n",
        "\n",
        "# Run the app\n",
        "!npm install localtunnel\n",
        "!streamlit run app.py --server.port 8501 &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "2VmOpGYRuin0",
        "outputId": "81337206-50c0-49c5-b17d-0b91f115eb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a145c0899d7d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ]
}